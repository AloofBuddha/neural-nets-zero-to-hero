{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AloofBuddha/neural-nets-zero-to-hero/blob/main/makemore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q3rRgh2QUev-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p7wwl5k9Uev_",
        "outputId": "77775560-8cdf-434f-d07a-aafb4a5ebba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'names.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3451568748.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'names.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 32k example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'names.txt'"
          ]
        }
      ],
      "source": [
        "words = open('names.txt', 'r').read().splitlines()\n",
        "\n",
        "words[:10]\n",
        "\n",
        "# 32k example\n",
        "len(words)\n",
        "\n",
        "# min length 2\n",
        "min(len(w) for w in words)\n",
        "\n",
        "# max length 15\n",
        "max(len(w) for w in words)\n",
        "\n",
        "# we want to think about the bigrams - what is the frequency of any given letter being followed by another letter\n",
        "\n",
        "# break down word to bigrams\n",
        "b = {} # keep track of counts\n",
        "for w in words:\n",
        "    # special characters for 'start' / 'end'\n",
        "    chs = ['<S>'] + list(w) + ['<E>']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        bigram = (ch1, ch2)\n",
        "        b[bigram] = b.get(bigram, 0) + 1\n",
        "\n",
        "counts = sorted(b.items(), key=lambda kv: -kv[1])\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPLXzJPRUewA"
      },
      "outputs": [],
      "source": [
        "words[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8SwchU6UewB"
      },
      "outputs": [],
      "source": [
        "# bigram count matrix\n",
        "N = torch.zeros((27, 27), dtype=torch.int32)\n",
        "\n",
        "# list of chars a-z\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "\n",
        "# lookup table char -> index in a-z (including '.' delimiter character for first)\n",
        "stoi = {s: i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "\n",
        "# we want a index -> char table as well\n",
        "itos = {i: s for s,i in stoi.items()}\n",
        "\n",
        "for w in words:\n",
        "    # special characters for 'start' / 'end'\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        ix1 = stoi[ch1]\n",
        "        ix2 = stoi[ch2]\n",
        "        bigram = (ch1, ch2)\n",
        "        N[ix1, ix2] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yinsGeZHUewB"
      },
      "outputs": [],
      "source": [
        "# visualizer - we have counts for our entire data set that captures all the bigrams, including metacharacters like 'start token' and 'end token'\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.imshow(N, cmap='Blues')\n",
        "for i in range(27):\n",
        "    for j in range(27):\n",
        "        chstr = itos[i] + itos[j]\n",
        "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
        "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne7TTdpOUewB"
      },
      "source": [
        "Even the most basic analysis can tell us things like:\n",
        "\n",
        "- The most frequent start letter is 'a'\n",
        "- the most frequent end letters are 'a' and 'n'\n",
        "- 'an' is our most frequent bigram\n",
        "- some of our data points are pointless - a word will never start with the `<E>` token for example, nor end with `<S>` - room to optimize!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOeYfKEQUewC"
      },
      "source": [
        "N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gfRdKhqUewC"
      },
      "outputs": [],
      "source": [
        "# N is our 27 x 27 nigram tensor\n",
        "N\n",
        "# our first row represents the counts for [aa, ab, ac, ad ... a.]\n",
        "N[0]\n",
        "\n",
        "# cast our sample into floats (was int32) and normalize by total counts\n",
        "p = N[0].float()\n",
        "p = p / p.sum()\n",
        "# p now represents the frequencies of a followed by another character, as compared against all characters, summing in total to 1\n",
        "p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdszLYNxUewC"
      },
      "outputs": [],
      "source": [
        "# for what we want to do next we need predictable random #s\n",
        "g = torch.Generator().manual_seed(2147483647) # manual seed is just to make this 'trackable' and not change on each run\n",
        "p = torch.rand(3, generator=g)\n",
        "# p holds 3 random #s now between 0 and 1\n",
        "p = p / p.sum()\n",
        "# p now equals 3 random normalized values (sum to 1)\n",
        "print(p)\n",
        "# now we can use this random distro split in 3 to produce data matching that distro using torch.multinomial\n",
        "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g)\n",
        "# by setting the sample size to 1 we can now get a single value from [0, 1, 2] with the generated distribution\n",
        "ix.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txeB1I63UewD"
      },
      "outputs": [],
      "source": [
        "# optimization - turn out predictions into a tensor instead of doing it by hand each time\n",
        "# N+1 is model smoothing so nll is never -Infinity\n",
        "P = (N+1).float()\n",
        "\n",
        "# understanding sum/keepdim\n",
        "# S = P.sum() # will sum the entire 27 x 27 matrix and return a scalar\n",
        "# S = P.sum(0) # will sum the matrix by col and return a 27 elem array\n",
        "# S = P.sum(1) # will sum the matrix by row and return a 27 elem array\n",
        "# S = P.sum(0, keepdim=True) # will sum the matrix by col and return a 1 x 27 matrix\n",
        "# S = P.sum(1, keepdim=True) # will sum the matrix by row and return a 27 x 1 matrix\n",
        "\n",
        "# ultimately this is what we want for matrix multipliction because it is transposed\n",
        "# remember - broadcasting rules\n",
        "# 27 x 27\n",
        "# 27 x 1\n",
        "# our final matrix S has 27 cols, each with a single row holding the sum of the equivalent row at P\n",
        "print(P.shape, S.shape)\n",
        "\n",
        "# each row represents a % frequency for that letter followed by all other letters (normalized)\n",
        "P /= P.sum(1, keepdim=True)\n",
        "\n",
        "\n",
        "# frequency per letter map\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.imshow(P, cmap='Blues')\n",
        "for i in range(27):\n",
        "    for j in range(27):\n",
        "        chstr = itos[i] + itos[j]\n",
        "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
        "        plt.text(j, i, \"{:.2f}\".format(P[i, j].item()), ha=\"center\", va=\"top\", color='gray')\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV0JZQZMUewD"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "# first for a baseline let's see how our bigram model performs with uniform distribution\n",
        "# (as if every letter is as likely to follow as any other)\n",
        "\n",
        "names = []\n",
        "\n",
        "for i in range(30):\n",
        "  ix = 0\n",
        "  name = []\n",
        "\n",
        "  while True:\n",
        "    p = torch.ones(27) / 27.0\n",
        "    # given that frequency distribution, get a random index\n",
        "    ix = torch.multinomial(p, num_samples=1,\n",
        "                           replacement=True, generator=g).item()\n",
        "    name.append(itos[ix])\n",
        "    # stop when the next random letter chosen is an end char symbol (. = <E>)\n",
        "    if ix == 0:\n",
        "      names.append(''.join(name))\n",
        "      break\n",
        "\n",
        "# wow, these are some pretty bad names. it has no concept of length\n",
        "names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhX_VrwXUewD"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "# now let's apply this to our bigram problem\n",
        "# start at 0 (. = <S>)\n",
        "names = []\n",
        "\n",
        "for i in range(30):\n",
        "  ix = 0\n",
        "  name = []\n",
        "\n",
        "  while True:\n",
        "    p = P[ix]\n",
        "    # given that frequency distribution, get a random index\n",
        "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    name.append(itos[ix])\n",
        "    # stop when the next random letter chosen is an end char symbol (. = <E>)\n",
        "    if ix == 0:\n",
        "      names.append(''.join(name))\n",
        "      break\n",
        "\n",
        "names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F_2F8L_UewD"
      },
      "source": [
        "\n",
        "these names are a bit better just because of the bigram frequency approach\n",
        "\n",
        "names are only so long and often start or end with an a or d as the model predicts\n",
        "\n",
        "but these don't actually read like names, especially the single letter ones\n",
        "\n",
        "How can we *train* our model to get better results using NN?\n",
        "\n",
        "Well first things first we need a way to measure our model's performance - NLL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb8yVgGeUewE"
      },
      "outputs": [],
      "source": [
        "# GOAL: maximize likelihood of the data w.r.t. model parameters (statistical modeling)\n",
        "# equivalent to maximizing the log likelihood (because log is monotonic)\n",
        "# equivalent to minimizing the negative log likelihood\n",
        "# equivalent to minimizing the average negative log likelihood\n",
        "\n",
        "# log(a*b*c) = log(a) + log(b) + log(c)\n",
        "log_likelihood = 0.0\n",
        "n = 0\n",
        "\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    prob = P[ix1, ix2]\n",
        "    logprob = torch.log(prob)\n",
        "    log_likelihood += logprob\n",
        "    n += 1\n",
        "    # print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
        "\n",
        "print(f'{log_likelihood=}')\n",
        "nll = -log_likelihood\n",
        "print(f'{nll=}')\n",
        "print(f'{nll/n}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CF9DImRUewE"
      },
      "source": [
        "Now let's cast our bigram model into the language of neural nets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozNKVHQ7UewE"
      },
      "outputs": [],
      "source": [
        "# create the training set of bigrams (x,y)\n",
        "xs, ys = [], []\n",
        "\n",
        "for w in words[:1]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    print(ch1, ch2)\n",
        "    xs.append(ix1)\n",
        "    ys.append(ix2)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "\n",
        "xs, ys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSfzt7OJUewE"
      },
      "source": [
        "our x -> y are both represented as integer mappings, so \".emma\" = [value for . (0), value for 'e' (5) ...]\n",
        "\n",
        "we have a total of 27 characters\n",
        "\n",
        "integers don't make so much sense to feed as the value into the input layer of a NN so we will utilize 'one hot encoding'\n",
        "\n",
        "in one hot encoding we represent the entire datatype as an array of length 27 and 0 for all indices except the one that matches\n",
        "\n",
        "so e = 5 = [0 0 0 0 1 0 0 0 ...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvjjB8EiUewE"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "xenc = F.one_hot(xs, num_classes=27).float()\n",
        "xenc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2RvzACRUewE"
      },
      "outputs": [],
      "source": [
        "plt.imshow(xenc)\n",
        "\n",
        "xenc.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9CPKXUjUewE"
      },
      "outputs": [],
      "source": [
        "W = torch.randn((27, 27))\n",
        "\n",
        "# @ is pytorch for matrix mult\n",
        "# (5, 27) @ (27, 27) = (27, 5)\n",
        "# results in a 5 x 27 array representng input layer \"emma\" (xs) modified by ws (weights)\n",
        "# exp is our way of handling negative numbers. Each value now represents the count\n",
        "# logits - log counts\n",
        "logits = xenc @ W\n",
        "# this is now equivalent to our counts matrix N\n",
        "counts = logits.exp()\n",
        "# probabilities the given letter follows\n",
        "probs = counts / counts.sum(1, keepdim=True)\n",
        "# represents how likely each character is to come next\n",
        "probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7X4M_Y_UewE"
      },
      "source": [
        "this is the simplest possible NN. It is an input layer, one hidden layer to turn it into a 27 and an output layer to transform it back to the input size. the input it modified by the weights resulting in the output\n",
        "\n",
        "Ex:\n",
        "\n",
        ". -> e\n",
        "input is . -> 0 -> one hot encoding -> probs of next letter:\n",
        "\n",
        "[0.0214, 0.0210, 0.0137, 0.0787, 0.0103, 0.0079, 0.0352, 0.0188, 0.0283,\n",
        "0.2689, 0.0029, 0.0203, 0.0246, 0.0863, 0.0320, 0.0099, 0.0078, 0.0348,\n",
        "0.0086, 0.0307, 0.0338, 0.0311, 0.0432, 0.0131, 0.0204, 0.0438, 0.0527]\n",
        "\n",
        "prob . -> . is 2%\n",
        "     . -> m is 8%\n",
        "\n",
        "sums to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WEt6f18UewF"
      },
      "outputs": [],
      "source": [
        "nlls = torch.zeros(5)\n",
        "for i in range(5):\n",
        "  # i-th bigram:\n",
        "  x = xs[i].item()  # input character index\n",
        "  y = ys[i].item()  # label character index\n",
        "  print('--------')\n",
        "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
        "  print('input to the neural net:', x)\n",
        "  print('output probabilities from the neural net:', probs[i])\n",
        "  print('label (actual next character):', y)\n",
        "  p = probs[i, y]\n",
        "  print('probability assigned by the net to the the correct character:', p.item())\n",
        "  logp = torch.log(p)\n",
        "  print('log likelihood:', logp.item())\n",
        "  nll = -logp\n",
        "  print('negative log likelihood:', nll.item())\n",
        "  nlls[i] = nll\n",
        "\n",
        "print('=========')\n",
        "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEFIupvPUewF"
      },
      "source": [
        "So we can evaluate the negative log likelihood of each example based on the input weights\n",
        "\n",
        "so how do we now run this repeatedly on itself so we can minimize nll?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2UqxREJUewF"
      },
      "outputs": [],
      "source": [
        "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
        "\n",
        "W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_ey7bsPUewF"
      },
      "outputs": [],
      "source": [
        "# forward pass\n",
        "# input to the network: one-hot encoding\n",
        "xenc = F.one_hot(xs, num_classes=27).float()\n",
        "logits = xenc @ W  # predict log-counts\n",
        "counts = logits.exp()  # counts, equivalent to N\n",
        "# probabilities for next character\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(5), ys].log().mean()\n",
        "\n",
        "loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-LOnwKwUewF"
      },
      "outputs": [],
      "source": [
        "# backward pass\n",
        "W.grad = None  # set to zero the gradient\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a4xVjyQUewF"
      },
      "source": [
        "So what did we do?\n",
        "\n",
        "example: . -> e\n",
        "\n",
        "fed input . -> 0 -> one hot encode -> applied weights (randomized initially according to normal dist) -> resulting prob matrix guessing the next letter -> compared to actual next letter 'e' we get a loss function representing how well we did -> run a backwards pass of our NN to readjust the weights to get a slightly better loss function the next time the same example is run on this NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl_eUBWxUewF"
      },
      "outputs": [],
      "source": [
        "# now let's try it on the full dataset\n",
        "\n",
        "# create the dataset\n",
        "xs, ys = [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    xs.append(ix1)\n",
        "    ys.append(ix2)\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num = xs.nelement()\n",
        "print('number of examples: ', num)\n",
        "\n",
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27, 27), generator=g, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAdxao9mUewF"
      },
      "outputs": [],
      "source": [
        "# gradient descent\n",
        "for k in range(200):\n",
        "\n",
        "  # forward pass\n",
        "  # input to the network: one-hot encoding\n",
        "  xenc = F.one_hot(xs, num_classes=27).float()\n",
        "  logits = xenc @ W  # predict log-counts\n",
        "  counts = logits.exp()  # counts, equivalent to N\n",
        "  # probabilities for next character\n",
        "  probs = counts / counts.sum(1, keepdims=True)\n",
        "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "  print(loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  W.grad = None  # set to zero the gradient\n",
        "  loss.backward()\n",
        "\n",
        "  # update (-50 represents our learning rate)\n",
        "  W.data += -50 * W.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmHDYZr9UewF"
      },
      "outputs": [],
      "source": [
        "# finally, sample from the 'neural net' model\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  out = []\n",
        "  ix = 0\n",
        "  while True:\n",
        "\n",
        "    # ----------\n",
        "    # BEFORE:\n",
        "    # p = P[ix]\n",
        "    # ----------\n",
        "    # NOW:\n",
        "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
        "    logits = xenc @ W  # predict log-counts\n",
        "    counts = logits.exp()  # counts, equivalent to N\n",
        "    # probabilities for next character\n",
        "    p = counts / counts.sum(1, keepdims=True)\n",
        "    # ----------\n",
        "\n",
        "    ix = torch.multinomial(\n",
        "        p, num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(itos[ix])\n",
        "    if ix == 0:\n",
        "      break\n",
        "  print(''.join(out))\n",
        "\n",
        "# after one pass it's not significantly better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg9ufUwpUewG"
      },
      "source": [
        "ok so what did we learn here?\n",
        "\n",
        "originally we built a model from scratch using our insight of the bigram counts\n",
        "\n",
        "then we did the same thing but starting from a random set of weights and using gradient descent\n",
        "\n",
        "after 100 iterations of gradient descent we get the same results as the handcrafted strategy\n",
        "\n",
        "it effectively learns the bigram counts model"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}